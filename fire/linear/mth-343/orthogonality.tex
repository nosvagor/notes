\chapter{3 Orthogonality}

\section{3.1 Orthogonal Vectors and Subspaces}
\begin{itemize}
  \item []
  \subsection{Problems 6, 46, 47}
  \begin{enumerate}
    \minor{\item[6.] Find all vectors in \(R^3\) that are orthogonal to \((1,1,1)\)
      and \((1,-1,0)\). Produce an orthonormal basis from these vectors
      (mutually orthogonal unit vectors).}
      \linkpy{orthogonality}
      \begin{itemize}
        \item Simply taking the cross product between two vectors in \(\R^3\)
          yields a new vector that is normal to the plane containing them.
        \item \((1,1,1) \times (1,-1,0) = \aset{(1,1,-2)}\) yields us
          orthogonal columns.
        \item Dividing each vector by their norms yields orthonormal columns, i.e.,
          \[%%%%%%%%%%
          \begin{bmatrix}
            \frac{1}{\sqrt{3} } & \frac{1}{\sqrt{2} } & \frac{1}{\sqrt{6} } \\
            \frac{1}{\sqrt{3} } & -\frac{1}{\sqrt{2} } & \frac{1}{\sqrt{6} } \\
            \frac{1}{\sqrt{3} } & 0 & -\frac{1}{\sqrt{3} }
          \end{bmatrix}
          \]%%%%%%%%%%
      \end{itemize}

    \minor{\item[46.] Find \(\bm{A}^T \bm{A}\) if the columns of \tbm{A}
      are unit vectors, all mutually perpendicular.
    }
    \[%%%%%%%%%%
    \begin{bmatrix}
      \norm{\bm{a_1}}^2 & a_\perp & \cdots & a_\perp \\
      a_\perp & \norm{\bm{a}_2}^2 & a_\perp & \vdots \\
      \vdots & a_\perp & \ddots & a_\perp  \\
      a_\perp & \cdots & a_\perp &\norm{\bm{a}_n}^2
    \end{bmatrix} =
    \begin{bmatrix}
      1 & 0 & \cdots & 0 \\
      0 & 1 & 0 & \vdots \\
      \vdots & 0 & \ddots & 0  \\
      0 & \cdots & 0 & 1
    \end{bmatrix} = \bm{I}
    \]%%%%%%%%%%

    \minor{\item[47.] Construct a \(3 \times 3\) matrix \tbm{A} with no zero
      entries whose columns are mutually perpendicular. Compute \(\bm{A}^T
      \bm{A}\). Why is it a diagonal matrix?
    }

    \linkpy{orthogonality}
    \begin{itemize}
      \item Note: I tried to set up with random matrices each time, sometimes it
        fails and gives the zero matrix.
      \item The way I set up only yields a diagonal matrix with
      \(\bm{A}\bm{A}^T \), \(\bm{A}^T \bm{A} \) yields a symmetric matrix. Why?
    \end{itemize}

  \end{enumerate}
\end{itemize}

\section{3.2 Cosines and Projections onto Lines}
\begin{itemize}
  \item []

  \subsection{Projection Proof (class problem)}
  \begin{itemize}
    \item If I recall the problem correctly, we were requested to \aset{prove what
      \(\proj{v}{w}\) is equal to}.

    \item In my notes I have that a orthogonal projection occurs when the dot
      product between \tbm{v} and distance \tbm{w} from \tbm{v} is equal to
      zero. This follows from the definition of the inner product, i.e.,
      \[%%%%%%%%%%
      \lambda = \bm{v}^T \bm{w} = \norm{v} \norm{w} \cos \theta
      \]%%%%%%%%%%
    \item If \(\theta = 90^\circ\), then the vectors are perpendicular, i.e.,
      orthogonal. What we are missing is \aset{the distance} from \tbm{w} to
      \tbm{v}; the distance that yields an inner product of zero with a
      normalized \tbm{v} \aset{\textit{is the projection}}.
    \item This means we need a scaled version of \tbm{v}, let's call it
      \(\bm{v}\beta\), at which such inner product is equal to zero. At this
      point, the difference between \tbm{w} and \(\bm{v}\beta\) is exactly what
      we need in order to solve for a \(\beta\) that maintains an inner
      product of zero with the original vector \tbm{v}, i.e.,
      \begin{align*}
        \bm{v}^T (\bm{w}-\bm{v}\beta) &= 0 \\
        \bm{v}^T\bm{w}-\bm{v}^T \bm{v}\beta &= 0 \\
        \bm{v}^T \bm{v}\beta &= \bm{v}^T\bm{w} \\
        \beta &= \frac{\bm{v}^T\bm{w}}{\bm{v}^T \bm{v}}\\\\
        \then
        \proj{v}{w} = \bm{v}\beta &= \bm{v}\frac{\bm{v}^T\bm{w}}{\bm{v}^T \bm{v}}
      \end{align*}
    \item I've internalized this as a mapping of \tbm{w} onto
      \tbm{v} over a magnitude (the norm) of \tbm{v}.
      \begin{itemize}
        \item The mapping is important because it tells us the shortest
          distance from \tbm{w} onto \tbm{v}, i.e., when they are orthogonal.
        \item The magnitude is important, because it is the basis at which
          \tbm{w} is parallel to \tbm{v}, which when added mapping distance,
          yields \tbm{w}.
      \end{itemize}
  \end{itemize}

  \newpage
  \subsection{Problems 10, 13, 15}
  \begin{itemize}
    \minor{\item[10.] Is the projection matrix P invertible? Why or why
    not?
    }
    \begin{align*}
      \bm{P} &= \frac{\bm{aa}^T}{\bm{a}^T \bm{a}} \\
      C(\bm{P}) &= \bm{a} = \rank{\bm{P}} = n = 1
    \end{align*}
    \begin{itemize}
      \item The rank in equal to the number of columns, thus it is invertible.
    \end{itemize}


    \minor{\item[13.] Prove that the trace of \(\bm{P} = \frac{\bm{aa}^T}{\bm{a}^T\bm{a}}\) always equals 1.
    }
    \begin{align*}
      \tr{\bm{A}} &= \sum_{i=1}^{n} e_{ij} = e_{11} + e_{22} + \cdots + e_{nn} \\\\
    \then \tr{\bm{P}} &= \frac{\bm{a}_1 \bm{a}_1}{\bm{a}^T \bm{a}}
    +\cdots+
    \frac{\bm{a}_n \bm{a}_n}{\bm{a}^T \bm{a}} =
    \frac{\bm{a}^T  \bm{a}}{\bm{a}^T \bm{a}} = 1 \\
    \end{align*}

    \minor{\item[15.] Show that the length of \tbm{Ax} equals the length of
      \(\bm{A}^T \bm{x}\) if \(\bm{AA}^T = \bm{A}^T \bm{A}\).
    }
    \begin{align*}
      \norm{\bm{A} \bm{x}}^2 &= (\bm{Ax})^T (\bm{Ax}) = \bm{x}\bm{A}^T \bm{A}\bm{x} \\
      \norm{\bm{A}^T  \bm{x}}^2 &= (\bm{A}^T \bm{x})^T (\bm{A}^T \bm{x}) = \bm{x}\bm{A}\bm{A}^T \bm{x}\\
      \bm{xA}^T \bm{Ax} = \bm{xA}\bm{A}^T \bm{x} &\iff \bm{A}\bm{A}^T = \bm{A}^T \bm{A}
    \end{align*}
  \end{itemize}

\end{itemize}

\section{3.3 Projections and Least Squares}
\begin{itemize}
  \item[]

  \subsection{Problems 8, 19, 20}
  \begin{enumerate}
    \minor{\item[8.]  If \tbm{P} is the projection matrix onto a \(k\)-dimensional
      subspace \tbm{S} of the whole space \(\R^n\), what is the column space
      of \tbm{P} and what is its rank?
    }
    \begin{itemize}
      \item Given \(\bm{P} = \bm{A}(\bm{A}^T \bm{A})^{-1} \bm{A}^T \), then
        \tbm{P} will project any vector onto the image of \tbm{A}, if \tbm{A}
        has independent columns.
      \item This implies that \tbm{S} is the image of \tbm{A}, i.e.,
        \(C(\bm{A}) = \bm{S}\).
      \item Since the \(k\)-dimensional subspace \tbm{S} is the whole of
        \(\R^n\), and \(\rank{\bm{A}} = n \), then \(\aset{C(\bm{P}) = \bm{S} \land
        \rank{\bm{P}} = k} \)
    \end{itemize}


    \minor{\item[19.] If \(\bm{P}_C = \bm{A}(\bm{A}^T \bm{A})^{-1}\bm{A}^T\) is
      the projection onto the column space of \tbm{A}, what is the projection
      \(\bm{P}_R\) onto the row space?
    }
    \begin{itemize}
      \item \(\bm{P}_C = C(\bm{A}), \bm{P}_R = C(\bm{A}^T )\), thus, taking the
        transpose of every instance of \tbm{A} will yield the row space, i.e.,
      \[%%%%%%%%%%
        \bm{P}_R = \bm{A}^T \left( \bm{A}^{TT} \bm{A}^T\right)^{-1} \bm{A}^{TT} =
        \bm{A}^T (\bm{A}\bm{A}^T )^{-1}\bm{A}
      \]%%%%%%%%%%
    \end{itemize}

    \minor{\item[20.] If \tbm{P} is the projection onto the column space of
      \tbm{A}, what is the projection onto the left nullspace?
    }
    \begin{itemize}
      \item The column space and left null space are orthogonal to each other,
        thus the projection onto the left null space is simply \(\bm{I} - \bm{P}\).
    \end{itemize}




  \end{enumerate}

\end{itemize}

\section{3.4 Orthogonal Bases and Gram-Schmidt}
\begin{itemize}
  \item[]

  \subsection{Implementation of the Gram-Schmidt Process}
  \begin{itemize}
    \item
  \end{itemize}

  \subsection{Problems 10, 13, 18}
  \begin{enumerate}
    \minor{\item[10.] 10. If \(\bm{q}_1\) and \(\bm{q}_2\) are the outputs from
      Gram-Schmidt, what were the possible input vectors \tbm{a} and \tbm{b}?
    }

    \minor{\item[13.] Apply the Gram-Schmidt process to
      \[%%%%%%%%%%
        \bm{a} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix},
        \qquad
        \bm{b} = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix},
        \qquad
        \bm{c} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
      \]%%%%%%%%%%
      and write the result in the form \(\bm{A} = \bm{QR}\).
    }
    \minor{\item[18.] If \(\bm{A} = \bm{QR}\), find a simple formula for the
      projection matrix \tbm{P} onto the column space of \tbm{A}.
    }

  \end{enumerate}

\end{itemize}
