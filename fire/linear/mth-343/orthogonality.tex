\chapter{3 Orthogonality}

\section{3.1 Orthogonal Vectors and Subspaces}
\begin{itemize}
  \item []
  \subsection{Problems 6, 46, 47}
  \begin{enumerate}
    \minor{\item[6.] Find all vectors in \(R^3\) that are orthogonal to \((1,1,1)\)
      and \((1,-1,0)\). Produce an orthonormal basis from these vectors
      (mutually orthogonal unit vectors).}
      \linkpy{orthogonality}
      \begin{itemize}
        \item Simply taking the cross product between two vectors in \(\R^3\)
          yields a new vector that is normal to the plane containing them.
        \item \((1,1,1) \times (1,-1,0) = \aset{(1,1,-2)}\) yields us
          orthogonal columns.
        \item Dividing each vector by their norms yields orthonormal columns, i.e.,
          \[%%%%%%%%%%
          \begin{bmatrix}
            \frac{1}{\sqrt{3} } & \frac{1}{\sqrt{2} } & \frac{1}{\sqrt{6} } \\
            \frac{1}{\sqrt{3} } & -\frac{1}{\sqrt{2} } & \frac{1}{\sqrt{6} } \\
            \frac{1}{\sqrt{3} } & 0 & -\frac{1}{\sqrt{3} }
          \end{bmatrix}
          \]%%%%%%%%%%
      \end{itemize}

    \minor{\item[46.] Find \(\bm{A}^T \bm{A}\) if the columns of \tbm{A}
      are unit vectors, all mutually perpendicular.
    }
    \[%%%%%%%%%%
    \begin{bmatrix}
      \norm{\bm{a_1}}^2 & a_\perp & \cdots & a_\perp \\
      a_\perp & \norm{\bm{a}_2}^2 & a_\perp & \vdots \\
      \vdots & a_\perp & \ddots & a_\perp  \\
      a_\perp & \cdots & a_\perp &\norm{\bm{a}_n}^2
    \end{bmatrix} =
    \begin{bmatrix}
      1 & 0 & \cdots & 0 \\
      0 & 1 & 0 & \vdots \\
      \vdots & 0 & \ddots & 0  \\
      0 & \cdots & 0 & 1
    \end{bmatrix} = \bm{I}
    \]%%%%%%%%%%

    \minor{\item[47.] Construct a \(3 \times 3\) matrix \tbm{A} with no zero
      entries whose columns are mutually perpendicular. Compute \(\bm{A}^T
      \bm{A}\). Why is it a diagonal matrix?
    }

    \linkpy{orthogonality}
    \begin{itemize}
      \item Note: I tried to set up with random matrices each time, sometimes it
        fails and gives the zero matrix.
      \item The way I set up only yields a diagonal matrix with
      \(\bm{A}\bm{A}^T \), \(\bm{A}^T \bm{A} \) yields a symmetric matrix. Why?
    \end{itemize}

  \end{enumerate}
\end{itemize}

\section{3.2 Cosines and Projections onto Lines}
\begin{itemize}
  \item []

  \subsection{Projection Proof (class problem)}
  \begin{itemize}
    \item If I recall the problem correctly, we were requested to \aset{prove what
      \(\proj{v}{w}\) is equal to}.

    \item In my notes I have that a orthogonal projection occurs when the dot
      product between \tbm{v} and distance \tbm{w} from \tbm{v} is equal to
      zero. This follows from the definition of the inner product, i.e.,
      \[%%%%%%%%%%
      \lambda = \bm{v}^T \bm{w} = \norm{v} \norm{w} \cos \theta
      \]%%%%%%%%%%
    \item If \(\theta = 90^\circ\), then the vectors are perpendicular, i.e.,
      orthogonal. What we are missing is \aset{the distance} from \tbm{w} to
      \tbm{v}; the distance that yields an inner product of zero with a
      normalized \tbm{v} \aset{\textit{is the projection}}.
    \item This means we need a scaled version of \tbm{v}, let's call it
      \(\bm{v}\beta\), at which such inner product is equal to zero. At this
      point, the difference between \tbm{w} and \(\bm{v}\beta\) is exactly what
      we need in order to solve for a \(\beta\) that maintains an inner
      product of zero with the original vector \tbm{v}, i.e.,
      \begin{align*}
        \bm{v}^T (\bm{w}-\bm{v}\beta) &= 0 \\
        \bm{v}^T\bm{w}-\bm{v}^T \bm{v}\beta &= 0 \\
        \bm{v}^T \bm{v}\beta &= \bm{v}^T\bm{w} \\
        \beta &= \frac{\bm{v}^T\bm{w}}{\bm{v}^T \bm{v}}\\\\
        \then
        \proj{v}{w} = \bm{v}\beta &= \bm{v}\frac{\bm{v}^T\bm{w}}{\bm{v}^T \bm{v}}
      \end{align*}
    \item I've internalized this as a mapping of \tbm{w} onto
      \tbm{v} over a magnitude (the norm) of \tbm{v}.
      \begin{itemize}
        \item The mapping is important because it tells us the shortest
          distance from \tbm{w} onto \tbm{v}, i.e., when they are orthogonal.
        \item The magnitude is important, because it is the basis at which
          \tbm{w} is parallel to \tbm{v}, which when added mapping distance,
          yields \tbm{w}.
      \end{itemize}
  \end{itemize}

  \newpage
  \subsection{Problems 10, 13, 15}
  \begin{itemize}
    \minor{\item[10.] Is the projection matrix P invertible? Why or why
    not?
    }
    \begin{align*}
      \bm{P} &= \frac{\bm{aa}^T}{\bm{a}^T \bm{a}} \\
      C(\bm{P}) &= \bm{a} = \rank{\bm{P}} = n = 1
    \end{align*}
    \begin{itemize}
      \item The rank in equal to the number of columns, thus it is invertible.
    \end{itemize}


    \minor{\item[13.] Prove that the trace of \(\bm{P} = \frac{\bm{aa}^T}{\bm{a}^T\bm{a}}\) always equals 1.
    }
    \begin{align*}
      \tr{\bm{A}} &= \sum_{i=1}^{n} e_{ij} = e_{11} + e_{22} + \cdots + e_{nn} \\\\
    \then \tr{\bm{P}} &= \frac{\bm{a}_1 \bm{a}_1}{\bm{a}^T \bm{a}}
    +\cdots+
    \frac{\bm{a}_n \bm{a}_n}{\bm{a}^T \bm{a}} =
    \frac{\bm{a}^T  \bm{a}}{\bm{a}^T \bm{a}} = 1 \\
    \end{align*}

    \minor{\item[15.] Show that the length of \tbm{Ax} equals the length of
      \(\bm{A}^T \bm{x}\) if \(\bm{AA}^T = \bm{A}^T \bm{A}\).
    }
    \begin{align*}
      \norm{\bm{A} \bm{x}}^2 &= (\bm{Ax})^T (\bm{Ax}) = \bm{x}\bm{A}^T \bm{A}\bm{x} \\
      \norm{\bm{A}^T  \bm{x}}^2 &= (\bm{A}^T \bm{x})^T (\bm{A}^T \bm{x}) = \bm{x}\bm{A}\bm{A}^T \bm{x}\\
      \bm{xA}^T \bm{Ax} = \bm{xA}\bm{A}^T \bm{x} &\iff \bm{A}\bm{A}^T = \bm{A}^T \bm{A}
    \end{align*}
  \end{itemize}

\end{itemize}

\section{3.3 Projections and Least Squares}
\begin{itemize}
  \item[]

  \subsection{Problems 8, 19, 20}
  \begin{enumerate}
    \minor{\item[8.]  If \tbm{P} is the projection matrix onto a \(k\)-dimensional
      subspace \tbm{S} of the whole space \(\R^n\), what is the column space
      of \tbm{P} and what is its rank?
    }
    \begin{itemize}
      \item Given \(\bm{P} = \bm{A}(\bm{A}^T \bm{A})^{-1} \bm{A}^T \), then
        \tbm{P} will project any vector onto the image of \tbm{A}, if \tbm{A}
        has independent columns.
      \item This implies that \tbm{S} is the image of \tbm{A}, i.e.,
        \(C(\bm{A}) = \bm{S}\).
      \item Since the \(k\)-dimensional subspace \tbm{S} is the whole of
        \(\R^n\), and \(\rank{\bm{A}} = n \), then \(\aset{C(\bm{P}) = \bm{S} \land
        \rank{\bm{P}} = k} \)
    \end{itemize}


    \minor{\item[19.] If \(\bm{P}_C = \bm{A}(\bm{A}^T \bm{A})^{-1}\bm{A}^T\) is
      the projection onto the column space of \tbm{A}, what is the projection
      \(\bm{P}_R\) onto the row space?
    }
    \begin{itemize}
      \item \(\bm{P}_C = C(\bm{A}), \bm{P}_R = C(\bm{A}^T )\), thus, taking the
        transpose of every instance of \tbm{A} will yield the row space, i.e.,
      \[%%%%%%%%%%
        \bm{P}_R = \bm{A}^T \left( \bm{A}^{TT} \bm{A}^T\right)^{-1} \bm{A}^{TT} =
        \bm{A}^T (\bm{A}\bm{A}^T )^{-1}\bm{A}
      \]%%%%%%%%%%
    \end{itemize}

    \minor{\item[20.] If \tbm{P} is the projection onto the column space of
      \tbm{A}, what is the projection onto the left nullspace?
    }
    \begin{itemize}
      \item The column space and left null space are orthogonal to each other,
        thus the projection onto the left null space is simply \(\bm{I} - \bm{P}\).
    \end{itemize}




  \end{enumerate}

\end{itemize}

\section{3.4 Orthogonal Bases and Gram-Schmidt}
\begin{itemize}
  \item[]

  \subsection{Implementation of the Gram-Schmidt Process}
  \link{https://github.com/nosvagor/notes/blob/main/fire/linear/mth-343/gramSchmidt.ipynb}{Implementing Gram-Schmidt}
  \vspace{18pt}

  \subsection{Problems 10, 13, 18}
  \begin{enumerate}
    \minor{\item[10.] 10. If \(\bm{q}_1\) and \(\bm{q}_2\) are the outputs from
      Gram-Schmidt, what were the possible input vectors \tbm{a} and \tbm{b}?
    }
    \begin{itemize}
      \item Any two linearly independent vectors.
    \end{itemize}


    \minor{\item[13.] Apply the Gram-Schmidt process to
      \[%%%%%%%%%%
        \bm{a} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix},
        \qquad
        \bm{b} = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix},
        \qquad
        \bm{c} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
      \]%%%%%%%%%%
      and write the result in the form \(\bm{A} = \bm{QR}\).
    }
    \[%%%%%%%%%%
    \bm{A} = \begin{bmatrix}
      0 & 0 & 1 \\
      0 & 1 & 1 \\
      1 & 1 & 1
    \end{bmatrix}
    \]%%%%%%%%%%
    \begin{itemize}
      \item All that is needed is a permutation to achieve orthonormal
        columns; switching rows 1 and 3 will achieve this, yielding \tbm{Q}:
        \[%%%%%%%%%%
        \bm{Q} = \begin{bmatrix}
          0 & 0 & \aset{1} \\
          0 & 1 & 0 \\
          \aset{1} & 0 & 0
        \end{bmatrix}
        \]%%%%%%%%%%
      \item Thus,
        \[%%%%%%%%%%
        \bm{A} = \bm{QR} \to
        \begin{bmatrix}
          0 & 0 & 1 \\
          0 & 1 & 1 \\
          1 & 1 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
          0 & 0 & \aset{1} \\
          0 & 1 & 0 \\
          \aset{1} & 0 & 0
        \end{bmatrix}
        \begin{bmatrix}
          1 & 1 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1
        \end{bmatrix}
      \]%%%%%%%%%%
    \end{itemize}

    \minor{\item[18.] If \(\bm{A} = \bm{QR}\), find a simple formula for the
      projection matrix \tbm{P} onto the column space of \tbm{A}.
    }
    \begin{itemize}
      \item I was lost on this one, but I wanted to know how to do it.
        Unfortunately, while looking for help I came across one of
        the \link{https://www.math.colostate.edu/~clayton/teaching/m215s10/homework/hw7solutions.pdf}{solutions
        documents} again.
      \item Normally I would just do a different problem, but I wanted to write
        it out, for personal reference.
    \end{itemize}
    \begin{align*}
      \bm{A} = \bm{QR} &\then \aset{\bm{A}^T  \bm{A}} = (\bm{QR})^T (\bm{QR}) = \bm{R}^T \bm{Q}^T \bm{QR} = \aset{\bm{R}^T \bm{R}} \\
      \aset{\bm{P}} &= \aset{\bm{A}(\bm{A}^T \bm{A})^{-1} \bm{A}^T } \\
             &\then
             \bm{QR}(\bm{R}^T \bm{R})^{-1} (\bm{QR})^T = \bm{QRR}^{-1}(\bm{R}^T )^{-1} \bm{R}^T \bm{Q}^T = \bm{QQ}^T = \bm{I} \\
             &\then \TT{\bm{P}^2} = \TT{\bm{P}}~\text{\true{}}
    \end{align*}

  \end{enumerate}
\end{itemize}

\section{3.5 The Fast Fourier Transform}
\begin{itemize}
  \item[]

  \subsection{Problems 1, 2}
  \begin{itemize}
    \minor{\item[1.] What are \(\bm{F}^2\) and \(\bm{F}^4\) for the \(4 \times
      4\) Fourier matrix \tbm{F}?
    }
    \begin{align*}
      \bm{F}^2 &=
      \begin{bmatrix}
        1 & 1 & 1 & 1 \\
        1 & i & i^2 & i^3 \\
        1 & i^2 & i^4 & i^6 \\
        1 & i^3 & i^6 & i^9
      \end{bmatrix}
      \begin{bmatrix}
        1 & 1 & 1 & 1 \\
        1 & i & i^2 & i^3 \\
        1 & i^2 & i^4 & i^6 \\
        1 & i^3 & i^6 & i^9
      \end{bmatrix} \\
      &=
      \begin{bmatrix}
        1 & 1 & 1 & 1 \\
        1 & \RR{i} & \BB{-1} & \BB{-i} \\
        1 & \BB{-1} & \RR{1} & \BB{-1} \\
        1 & \BB{-i} & \BB{-1} & \RR{i}
      \end{bmatrix}
      \begin{bmatrix}
        1 & 1 & 1 & 1 \\
        1 & \RR{i} & \BB{-1} & \BB{-i} \\
        1 & \BB{-1} & \RR{1} & \BB{-1} \\
        1 & \BB{-i} & \BB{-1} & \RR{i}
      \end{bmatrix} \\
      &=
      \begin{bmatrix}
          \aset{4} & 0 & 0 & 0\\
          0 & 0 & 0 & \aset{4}\\
          0 & 0 & \aset{4} & 0\\
          0 & \aset{4} & 0 & 0
      \end{bmatrix} \\\\
      \bm{F}^4 &=
      \begin{bmatrix}
          \aset{4} & 0 & 0 & 0\\
          0 & 0 & 0 & \aset{4}\\
          0 & 0 & \aset{4} & 0\\
          0 & \aset{4} & 0 & 0
      \end{bmatrix}
      \begin{bmatrix}
          \aset{4} & 0 & 0 & 0\\
          0 & 0 & 0 & \aset{4}\\
          0 & 0 & \aset{4} & 0\\
          0 & \aset{4} & 0 & 0
      \end{bmatrix}
      =
      \begin{bmatrix}
          \aset{16} & 0 & 0 & 0\\
          0 & \aset{16} & 0 & 0\\
          0 & 0 & \aset{16} & 0\\
          0 & 0 & 0 & \aset{16}
      \end{bmatrix}
    \end{align*}

    \minor{\item[2.] Find a permutation \tbm{P} of the columns of \tbm{F} that produces
      \(\bm{FP} = \bm{\overline{F}}~(n \times n)\). Combine with
      \(\bm{F\overline{F}} = n\bm{I}\) to find \(\bm{F}^2\) and \(\bm{F}^4\)
      for the \(n \times n\) Fourier matrix.
    }
    \begin{align*}
      \bm{P} &=
      \begin{bmatrix}
        \aset{1} & 0 & \cdots & 0 & 0\\
        0 & 0 & \cdots & 0 &\aset{1} \\
        \vdots & \vdots & \iddots & \aset{\iddots} & 0\\
        0 & 0 & \aset{\iddots} & \iddots & \vdots \\
      0 & \aset{1} & 0 & \cdots & 0 \\
      \end{bmatrix} \\
      \bm{F\overline{F}} &=
      \aset{n}
      \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0 \\
        0 & \cdots & 0 & 1 \\
      \end{bmatrix} \quad (n \times n) \\\\
      \bm{F}^2 =
      \begin{bmatrix}
        \aset{n} & 0 & \cdots & 0 \\
        0 & \aset{n} & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0 \\
        0 & \cdots & 0 & \aset{n} \qquad
      \end{bmatrix}
                         &\quad
      \bm{F}^4 =
      \begin{bmatrix}
        \aset{n^2} & 0 & \cdots & 0 \\
        0 & \aset{n^2} & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0 \\
        0 & \cdots & 0 & \aset{n^2} \qquad
      \end{bmatrix}
    \end{align*}
  \end{itemize}


  \subsection{Problems 5, 6}
  \begin{itemize}
    \minor{\item[5.] Find all solutions to the equation \(e^{ix} = -1\), and
      all solutions to \(e^{i\theta} = i\).
    }
    \begin{align*}
      e^{ix} &= -1 &&& e^{i\theta} &= i \\
             &\then \cos x + i \sin x = -1 + 0i
             &&&
             &\then \cos \theta + i \sin \theta = 0 + 1i \\
             &\then \cos x = -1 \land \sin x = 0
             &&&
             &\then \cos \theta = 0 \land \sin \theta = 1 \\
             &\then \aset{x = \pi + 2k\pi, \quad k \in \Z}
             &&&
             &\then \aset{\theta = \frac{\pi}{2} + 2k\pi, \quad k \in \Z}
    \end{align*}


    \minor{\item[6.] What are the square and square roots of \(w_{128}\), the
      primitive \(128^{th}\) root of \(1\)?
    }
    \begin{align*}
      \mid w^2_n = w_m &\iff m = \frac{1}{2}n \\
      \then w_{128}^2 = \aset{w_{64}} &\land \sqrt{w_{128}}= \aset{w_{256}} \\\\
      w_{128} = e^{2\pi i / 128} &= \cos \frac{2\pi}{128} + i\sin  \frac{2\pi}{128} \\
                            &= \cos 0 + i\sin 0 = 1 + i 0 = \aset{1}
    \end{align*}
  \end{itemize}

\end{itemize}
